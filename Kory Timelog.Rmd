---
title: "Kory Time Log"
author: "Kory Rosen"
date: "2024-01-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
```

```{r}
timelog <- data.frame(activityID = character(), week=numeric(), dateStarted = lubridate::ymd(), dateCompleted = lubridate::ymd(), activityType = factor(), description = character(), numberOfMinutesWorked = numeric())
```

# Time Log done using a dataframe

```{r}
append_activity <- function(timelog, activity) {
  if (activity[[1]] %in% timelog$activityID) {
    message("Activity ID already exists in the timelog dataframe. Skipping append.")
    return(timelog)
  }
    dateCompleted <- if (is.null(activity[[4]])) NA else as.Date(activity[[4]])
  
  timelog <- rbind(timelog, data.frame(activityID = activity[[1]], 
                                       week = activity[[2]],
                                       dateStarted = as.Date(activity[[3]]),
                                       dateCompleted = dateCompleted,
                                       activityType = as.factor(activity[[5]]),
                                       description = activity[[6]],
                                       numberOfMinutesWorkedThatDate = activity[[7]],
                                       totalNumberOfMinutesWorked = activity[[8]],
                                       stringsAsFactors = FALSE))
  return(timelog)
}

```

```{r}
activity1 <- list("Activity 1", 1, "2024-01-24", "2024-01-24", "Research", "Thinking about what dataset I wanted to choose during class and how I wanted to approach it.", 20, 20)
timelog <- append_activity(timelog, activity1)
rm(activity1)
```

```{r}
activity2 <- list("Activity 2", 1, "2024-01-24", NULL, "Research", "Note from 01/24/2024: I did this part in class and after class on this date. Given that I wanted to do Supreme Court data, I started to search for how to find a dataset online. I first found a dataset on Kaggle, but the issue would be that I would have to scrape the actual opinions from the website, which would be a pain. Then, I found CourtListener, which would be a great choice, but I had to figure out how I would download the data since the bulk data would be huge to download.", 120, 120)
timelog <- append_activity(timelog, activity2)
rm(activity2)
```

```{r}
timelog[timelog$activityID == "Activity 2",]$description <- paste(timelog[timelog$activityID == "Activity 2",]$description, " Update from 02/01/2024: I gave up using CourtListener bulk data because it would come to over 50 GB of data and my comptuer cannot hold that, so had to find a different way.", sep="")
timelog[timelog$activityID == "Activity 2",]$numberOfMinutesWorkedThatDate <- 180
timelog[timelog$activityID == "Activity 2",]$totalNumberOfMinutesWorked <- timelog[timelog$activityID == "Activity 2",]$totalNumberOfMinutesWorked + timelog[timelog$activityID == "Activity 2",]$numberOfMinutesWorkedThatDate
timelog[timelog$activityID == "Activity 2",]$dateCompleted <- "2024-02-01"
```

```{r}
activity3 <- list("Activity 3", 3, "2024-02-04", "2024-02-04", "Research", "Note from 02/04/2024: I tried to use this dataset on Hugging Face regarding Supreme Court Cases, but could not get it to work. Tried for about 3 hours", 180, 180)
timelog <- append_activity(timelog, activity3)
rm(activity3)
```


```{r}
activity4 <- list("Activity 4", 3, "2024-02-09", NULL, "Research", "Looked for another way to do this because I was keep stricking out without luck. Was Googling for APIs that had this for about an hour and a half.", 90, 90)
timelog <- append_activity(timelog, activity4)
rm(activity4)
```

```{r}
timelog[timelog$activityID == "Activity 4",]$description <- paste(timelog[timelog$activityID == "Activity 4",]$description, " Update from 02/19/2024: I found the CourtListener API, which seemed like the least memory intensive and computationally intensive way of doing this so far, so I will use this it seems. Based on testing for about 2 hours, the API does what I want it to do.", sep="")
timelog[timelog$activityID == "Activity 4",]$numberOfMinutesWorkedThatDate <- 120
timelog[timelog$activityID == "Activity 4",]$totalNumberOfMinutesWorked <- timelog[timelog$activityID == "Activity 4",]$totalNumberOfMinutesWorked + timelog[timelog$activityID == "Activity 4",]$numberOfMinutesWorkedThatDate
timelog[timelog$activityID == "Activity 4",]$dateCompleted <- "2024-02-19"
```

```{r}
activity5 <- list("Activity 5", 6, "2024-02-20", "2024-02-20", "Email Correspondence", "I was corresponding with Mike Lissener, the cofounder of the project. My email was the following: Hope all is well. I am a 4th year at Grinnell College majoring in computer science and political science and I need your help for my project in software development class. I am planning on doing a Natural Language Processing project on Supreme Court Cases from 2019-2022 and was wondering if there is any way to scrape more than 10 pages worth of data using the API because 10 pages isn't enough to get all the information.  I tried with bulk data but that is too much of a hassle. Thank you so much. He responded that is all he has as of now, so I went to find ways to use the API. ", 60, 60)
timelog <- append_activity(timelog, activity5)
rm(activity5)
```

```{r}
activity6 <- list("Activity 6", 6, "2024-02-21", "2024-02-21", "Research", "For 3 hours during the late afternoon (between 4 and 7), I was exploring how the DRF worked and how queries work within it.", 180, 180)
timelog <- append_activity(timelog, activity6)
rm(activity6)
```

```{r}
activity7 <- list("Activity 7", 6, "2024-02-23", "2024-02-23", "Data Collection", "Created a small snippet of the dataframe that I wanted to create. (Did this between 4pm and 8:30pm)", 270, 270)
timelog <- append_activity(timelog, activity7)
rm(activity7)
```

```{r}
activity8 <- list("Activity 8", 7, "2024-02-26", "2024-02-26", "Visualization Creation", "Created some sample visualizations that I could do and saw what worked and what did not work (Did this between 5pm and 9:00pm)", 240, 240)
timelog <- append_activity(timelog, activity8)
rm(activity8)
```

```{r}
activity9 <- list("Activity 9", 7, "2024-03-02", NULL, "Data Collection", "Created a script that scraped the whole amount of data we needed. First, I had to conceptually think of how I could do this without making that many API calls. This was the hardest part because I did not want to overrun the nonprofits servers, which I had done during my testing (Worked on it between 12:30pm and 5pm). Planning on doing testing later.", 240, 240)
timelog <- append_activity(timelog, activity9)
rm(activity9)
```

```{r}
timelog[timelog$activityID == "Activity 9",]$description <- paste(timelog[timelog$activityID == "Activity 9",]$description, " Update from 03/05/2024: Trials keep failing during the middle of the test run. Keep getting annoyed. Have to figure out a way to stop this from happening. I am implementing pickle files now and going to split it into shorter files. I worked on it today for about an hour so far. ", sep="")
timelog[timelog$activityID == "Activity 9",]$numberOfMinutesWorkedThatDate <- 60
timelog[timelog$activityID == "Activity 9",]$totalNumberOfMinutesWorked <- timelog[timelog$activityID == "Activity 9",]$totalNumberOfMinutesWorked + timelog[timelog$activityID == "Activity 9",]$numberOfMinutesWorkedThatDate
```

```{r}
timelog[timelog$activityID == "Activity 9",]$description <- paste(timelog[timelog$activityID == "Activity 9",]$description, " Update from 03/05/2024 (Later at night): I was so far in and my internet got disrupted. Need to think of a workaround for this.", sep="")
timelog[timelog$activityID == "Activity 9",]$dateCompleted <- "2024-03-05"
```

```{r}
activity10 <- list("Activity 10", 8, "2024-03-06", "2024-03-06", "Data Collection", "From 6pm to 11pm, I created the script with multiprocessing and failsafes using Google Collab+ for Clusters and recieved results.", 300, 300)
timelog <- append_activity(timelog, activity10)
rm(activity10)
```


```{r}
activity11 <- list("Activity 11", 8, "2024-03-07", NULL , "Data Collection", "From 6pm to 11pm, I created the script with multiprocessing and failsafes using Google Collab+ for Opinions and did not recieve results because there was an error.", 270, 270)
timelog <- append_activity(timelog, activity11)
rm(activity11)
```

```{r}
timelog[timelog$activityID == "Activity 11",]$description <- paste(timelog[timelog$activityID == "Activity 11",]$description, " Update from 03/08/2024: I fixed the error and got results. Took about 3 hours to run and 30 minutes to fix and it was done sporadically throughout the day. ", sep="")
timelog[timelog$activityID == "Activity 11",]$numberOfMinutesWorkedThatDate <- 210
timelog[timelog$activityID == "Activity 11",]$totalNumberOfMinutesWorked <- timelog[timelog$activityID == "Activity 11",]$totalNumberOfMinutesWorked + timelog[timelog$activityID == "Activity 11",]$numberOfMinutesWorkedThatDate
timelog[timelog$activityID == "Activity 11",]$dateCompleted <- "2024-03-08"
```

```{r}
activity12 <- list("Activity 12", 8, "2024-03-08", "2024-03-08" , "Data Collection", "From 7pm-7:30pm, I created the script with multiprocessing and failsafes using Google Collab+ for Authors and did recieved results", 30, 30)
timelog <- append_activity(timelog, activity12)
rm(activity12)
```

```{r}
activity13 <- list("Activity 13", 8, "2024-03-08", "2024-03-08" , "Data Collection", "From 7:30pm-8:00pm, I created the script with multiprocessing and failsafes using Google Collab+ for Docket ID and did recieved results", 30, 30)
timelog <- append_activity(timelog, activity13)
rm(activity13)
```

```{r}
activity14 <- list("Activity 14", 8, "2024-03-09", "2024-03-09" , "Data Transformation", "From 11am-8pm, Fixed the data and make it 'clean' by combining datasets, changing types, and getting it suitable to work in R.", 540, 540)
timelog <- append_activity(timelog, activity14)
rm(activity14)
```

```{r}
activity15 <- list("Activity 15", 9, "2024-03-10", "2024-03-10" , "Shiny App Creation", "From 1pm-9pm, I used the visualizations that I already had and adapted them into Shiny format and also created some new visualizations.", 480, 480)
timelog <- append_activity(timelog, activity15)
rm(activity15)
```


```{r}
activity16 <- list("Activity 16", 9, "2024-03-11", "2024-03-11" , "Data Transformation", "From 6am to 8am, I removed duplicates in my dataset using a function I had created that was inspired by fuzzy matching.", 120, 120)
timelog <- append_activity(timelog, activity16)
rm(activity16)
```

```{r}
save(timelog, file = "timelog.RData")
```
















