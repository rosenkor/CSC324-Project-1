{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsgU0pIqbB_6",
        "outputId": "d02eb682-cb3d-48f5-83e0-7ff03bf7269e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.4-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting reachability\n",
            "  Downloading reachability-0.1.4-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2) (1.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2) (3.1.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2) (2.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, reachability, datetime, bs4\n",
            "Successfully installed bs4-0.0.2 datetime-5.4 reachability-0.1.4 zope.interface-6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests bs4 pandas numpy datetime urllib3 tqdm reachability rpy2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdIKee21bNzj"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.courtlistener.com/api/rest/v3/\"\n",
        "api_key = \"163cea228fb27936988d579ed72fe787848e1866\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWDfYseOa6X0",
        "outputId": "1dafefe8-2bb4-43b5-e0b6-5a4a4ee3a350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching sub-opinions data:   0%|          | 1/18050 [00:02<11:16:36,  2.25s/page]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "502 Server Error: Bad Gateway for url: https://www.courtlistener.com/api/rest/v3/opinions/?id__gte=4454847&id__lte=4454851&author__gte=0&order_by=id&fields=plain_text,html,html_lawbox,html_columbia,html_anon_2020,xml_harvard,html_with_citations,opinions_cited,author_id,id,cluster_id\n",
            "502 Server Error: Bad Gateway for url: https://www.courtlistener.com/api/rest/v3/opinions/?id__gte=4398523&id__lte=4398526&author__gte=0&order_by=id&fields=plain_text,html,html_lawbox,html_columbia,html_anon_2020,xml_harvard,html_with_citations,opinions_cited,author_id,id,cluster_id\n",
            "502 Server Error: Bad Gateway for url: https://www.courtlistener.com/api/rest/v3/opinions/?id__gte=4435007&id__lte=4435010&author__gte=0&order_by=id&fields=plain_text,html,html_lawbox,html_columbia,html_anon_2020,xml_harvard,html_with_citations,opinions_cited,author_id,id,cluster_id\n",
            "502 Server Error: Bad Gateway for url: https://www.courtlistener.com/api/rest/v3/opinions/?id__gte=9297448&id__lte=9297451&author__gte=0&order_by=id&fields=plain_text,html,html_lawbox,html_columbia,html_anon_2020,xml_harvard,html_with_citations,opinions_cited,author_id,id,cluster_id\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as beautifulsoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urlencode\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import time\n",
        "from reachability import reachability\n",
        "import pickle\n",
        "\n",
        "def get_data_from_sub_opinions(api_key, url, sub_opinions_ids, excluded_ids, max_queries_per_hour=2000):\n",
        "    url = url + \"opinions/\"\n",
        "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
        "    opinions_data = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    query_count = 0\n",
        "\n",
        "    with tqdm(total=len(sub_opinions_ids), desc=\"Fetching sub-opinions data\", unit=\"page\") as pbar:\n",
        "        for seq in sub_opinions_ids:\n",
        "            params = {\n",
        "                \"id__gte\": seq[0],\n",
        "                \"id__lte\": seq[1],\n",
        "                \"author__gte\": 0,\n",
        "                \"order_by\": \"id\",\n",
        "                \"fields\": \"plain_text,html,html_lawbox,html_columbia,html_anon_2020,xml_harvard,html_with_citations,opinions_cited,author_id,id,cluster_id\",\n",
        "            }\n",
        "            while True:\n",
        "                try:\n",
        "                    # Check if the rate limit has been reached\n",
        "                    if query_count >= max_queries_per_hour:\n",
        "                        elapsed_time = time.time() - start_time\n",
        "                        if elapsed_time < 3600:  # 3600 seconds = 1 hour\n",
        "                            sleep_time = 3600 - elapsed_time\n",
        "                            print(f\"Rate limit reached. Waiting for {sleep_time:.2f} seconds...\")\n",
        "                            time.sleep(sleep_time)\n",
        "                            start_time = time.time()\n",
        "                            query_count = 0\n",
        "\n",
        "                    newurl = f\"{url}?{urlencode(params, safe=',')}\"\n",
        "                    response = requests.get(newurl, headers=headers)\n",
        "                    response.raise_for_status()\n",
        "                    data = response.json()\n",
        "                    opinions_data.extend(data[\"results\"])\n",
        "                    if not data[\"next\"]:\n",
        "                        break\n",
        "                    params[\"id__gte\"] = data[\"results\"][-1][\"id\"] + 1\n",
        "                    pbar.update(1)\n",
        "                    query_count += 1\n",
        "                    time.sleep(5)\n",
        "                except requests.exceptions.HTTPError as err:\n",
        "                    # If the request fails, wait 3 minutes and try again with the same parameters\n",
        "                    print(err)\n",
        "                    time.sleep(180)\n",
        "                    continue\n",
        "                except requests.exceptions.ConnectionError as err:\n",
        "                    reachable = reachability(timeout=None)\n",
        "                    while not reachable.is_online():\n",
        "                        print(\"Waiting for internet connection...\")\n",
        "                        time.sleep(120)\n",
        "                    continue\n",
        "                except Exception as err:\n",
        "                    print(\"Other error: \", err)\n",
        "                    break\n",
        "    return opinions_data\n",
        "\n",
        "# Open the pickled file\n",
        "with open('clusters_data.pickle', 'rb') as f:\n",
        "    clusters = pickle.load(f)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Extract the sub_opinions values from each dictionary\n",
        "sub_opinions = [item['sub_opinions'] for item in clusters if 'sub_opinions' in item]\n",
        "\n",
        "# Flatten the list of lists and convert to a set to get unique values\n",
        "unique_values = set(x for sublist in sub_opinions for x in sublist)\n",
        "\n",
        "# Convert the set to a sorted list\n",
        "result = sorted(list(unique_values))\n",
        "\n",
        "# Find the sequences of numbers\n",
        "sequences = []\n",
        "current_sequence = [result[0]]\n",
        "\n",
        "for i in range(1, len(result)):\n",
        "    if result[i] == result[i-1] + 1:\n",
        "        current_sequence.append(result[i])\n",
        "    else:\n",
        "        sequences.append(current_sequence)\n",
        "        current_sequence = [result[i]]\n",
        "\n",
        "sequences.append(current_sequence)\n",
        "\n",
        "# Format the sequences\n",
        "formatted_sequences = []\n",
        "for seq in sequences:\n",
        "    if len(seq) == 1:\n",
        "        formatted_sequences.append([seq[0], seq[0]])\n",
        "    else:\n",
        "        formatted_sequences.append([seq[0], seq[-1]])\n",
        "\n",
        "# Sort the formatted sequences by the difference between the two numbers in descending order\n",
        "sorted_sequences = sorted(formatted_sequences, key=lambda x: x[1] - x[0], reverse=True)\n",
        "\n",
        "# Call the get_data_from_sub_opinions function with the sorted sequences\n",
        "excluded_ids = []\n",
        "opinions_data = get_data_from_sub_opinions(api_key, url, sorted_sequences, excluded_ids, max_queries_per_hour=2000)\n",
        "\n",
        "# Save the fetched opinions data as a pickle file\n",
        "with open('/content/drive/MyDrive/opinions_data.pickle', 'wb') as f:\n",
        "    pickle.dump(opinions_data, f)\n",
        "\n",
        "print(\"Fetched opinions data saved as 'opinions_data.pickle'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}